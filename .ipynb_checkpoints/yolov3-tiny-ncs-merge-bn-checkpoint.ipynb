{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmark\n",
      "key:layer1-conv\n",
      "is caffe._caffe.BlobVec\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid index type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f872120df120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0mnet_deploy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaffe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeploy_proto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaffe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'benchmark'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mmerge_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_deploy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;31m#net_deploy.save(save_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f872120df120>\u001b[0m in \u001b[0;36mmerge_bn\u001b[0;34m(net, nob)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"merge layer {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"-bn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid index type"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import sys,os  \n",
    "caffe_root = '/data/ssd-caffe/py2_yolov3_caffe/'\n",
    "sys.path.insert(0, caffe_root + 'python')  \n",
    "import caffe\n",
    "\n",
    "#train_proto = 'yolov3-tiny-ncs-without-last-maxpool.prototxt'\n",
    "#train_model = 'snapshot/mobilenet_iter_7000.caffemodel'  #should be your snapshot caffemodel\n",
    "train_proto = 'yolov3.prototxt'\n",
    "deploy_proto = 'yolov3_merged_bn.prototxt'\n",
    "'''if(len(sys.argv) < 2):\n",
    "    print(\"please input caffemodel file from commandLine\")\n",
    "    exit(1)\n",
    "train_model = sys.argv[1]'''\n",
    "\n",
    "#deploy_proto = 'yolov3-tiny-ncs-without-last-maxpool-merg-batchnorm.prototxt'  \n",
    "#save_model = 'iter_7000_MobileNetSSD_deploy.caffemodel'\n",
    "#save_model = train_model.split('/')[-1] + \"MobileNetSSD_deploy.caffemodel\"\n",
    "save_model = \"generated_merge_yolov3_header_int.caffemodel\"\n",
    "save_weights = \"header_int_aaa_yolov3.weights\"\n",
    "#global buf #----why can not work ?\n",
    "\n",
    "def merge_bn(net, nob):\n",
    "    '''\n",
    "    merge the batchnorm, scale layer weights to the conv layer, to  improve the performance\n",
    "    var = var + scaleFacotr\n",
    "    rstd = 1. / sqrt(var + eps)\n",
    "    w = w * rstd * scale\n",
    "    b = (b - mean) * rstd * scale + shift\n",
    "    '''\n",
    "    buf = [0, 2, 0, 960000, 0]\n",
    "    for key in net.params.iterkeys():\n",
    "        print(\"key:\" + str(key))\n",
    "        if type(net.params[key]) is caffe._caffe.BlobVec:\n",
    "            print(\"is caffe._caffe.BlobVec\")\n",
    "            if key.endswith(\"-bn\") or key.endswith(\"-scale\"):\n",
    "                continue\n",
    "            else:\n",
    "                conv = net.params[key]\n",
    "                print(conv[0].data)\n",
    "                print(\"merge layer {0}\".format(conv))\n",
    "                if not net.params.has_key(key[:-5] + \"-bn\"):\n",
    "                    print(\"not need bn, copy w and b\")\n",
    "                    for i, w in enumerate(conv):\n",
    "                        #print(\"i={0}\".format(i))\n",
    "                        nob.params[key][i].data[...] = w.data\n",
    "                        #print(w.data.shape)\n",
    "                    temp_array = (conv[1].data).reshape(-1)                       \n",
    "                    buf = buf + temp_array.tolist()\n",
    "                    print(len(buf))\n",
    "                    temp_array = (conv[0].data).reshape(-1)                       \n",
    "                    buf = buf + temp_array.tolist()\n",
    "                    print(len(buf))\n",
    "                else:\n",
    "                    \n",
    "                    print(\"need bn, really start bn and scale\")\n",
    "                    bn = net.params[key[:-5] + \"-bn\"]\n",
    "                    scale = net.params[key[:-5] + \"-scale\"]\n",
    "                    wt = conv[0].data\n",
    "                    channels = wt.shape[0]\n",
    "                    bias = np.zeros(wt.shape[0])\n",
    "                    if len(conv) > 1:\n",
    "                        bias = conv[1].data\n",
    "                    mean = bn[0].data\n",
    "                    var = bn[1].data\n",
    "                    scalef = bn[2].data\n",
    "                    print('scalef = {0}'.format(scalef))\n",
    "\n",
    "                    scales = scale[0].data\n",
    "                    shift = scale[1].data\n",
    "\n",
    "                    if scalef != 0:\n",
    "                        scalef = 1. / scalef\n",
    "                    mean = mean * scalef\n",
    "                    var = var * scalef\n",
    "                    rstd = 1. / np.sqrt(var + 1e-5)\n",
    "                    rstd1 = rstd.reshape((channels,1,1,1))\n",
    "                    scales1 = scales.reshape((channels,1,1,1))\n",
    "                    wt = wt * rstd1 * scales1\n",
    "                    bias = (bias - mean) * rstd * scales + shift                   \n",
    "                    nob.params[key][0].data[...] = wt                   \n",
    "                    nob.params[key][1].data[...] = bias\n",
    "                    \n",
    "                    print(bias.shape)\n",
    "                    temp_array = bias.reshape(-1)\n",
    "                    buf = buf + temp_array.tolist()\n",
    "                    print(len(buf))\n",
    "                    \n",
    "                    print(wt.shape)\n",
    "                    temp_array = wt.reshape(-1)\n",
    "                    buf = buf + temp_array.tolist()\n",
    "                    print(len(buf))\n",
    "    \n",
    "    weights_header = np.array(buf[:5], dtype=np.int32)\n",
    "    weights_merged_bn = np.array(buf[5:], dtype=np.float32)\n",
    "    print(weights_header)\n",
    "    print(weights_merged_bn)\n",
    "    fp = open(save_weights, \"wb\")\n",
    "    weights_header.tofile(fp)\n",
    "    weights_merged_bn.tofile(fp)\n",
    "    #np.save('npsaved_file', weights_merged_bn)\n",
    "\n",
    "#net = caffe.Net(train_proto, train_model, caffe.TRAIN)\n",
    "caffe.set_mode_cpu()\n",
    "#net = caffe.Net(train_proto, 'Jenerated_nolastpooling.caffemodel', caffe.TEST)\n",
    "net = caffe.Net(train_proto, 'yolov3.caffemodel', caffe.TEST)\n",
    "net_deploy = caffe.Net(deploy_proto, caffe.TEST)  \n",
    "print('benchmark')\n",
    "merge_bn(net, net_deploy)\n",
    "#net_deploy.save(save_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(np.ndarray.tofile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
