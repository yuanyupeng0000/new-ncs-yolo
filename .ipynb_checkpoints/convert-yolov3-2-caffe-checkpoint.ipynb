{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dynamic module does not define init function (init_caffe)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-10e515810621>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/data/ssd-caffe/yolov3_caffe/python'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcaffe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/movidius/caffe/python/caffe/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpycaffe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSGDSolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNesterovSolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdaGradSolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRMSPropSolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdaDeltaSolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdamSolver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_caffe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_mode_cpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_mode_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_solver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_type_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_random_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_caffe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaffe_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/movidius/caffe/python/caffe/pycaffe.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_caffe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSGDSolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNesterovSolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdaGradSolver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mRMSPropSolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdaDeltaSolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdamSolver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcaffe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: dynamic module does not define init function (init_caffe)"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '/data/ssd-caffe/yolov3-caffe/python')\n",
    "import caffe  \n",
    "import numpy as np  \n",
    "from collections import OrderedDict\n",
    "from ConfigParser import ConfigParser\n",
    "\n",
    "class uniqdict(OrderedDict):\n",
    "    _unique = 0\n",
    "    def __setitem__(self, key, val):\n",
    "        if isinstance(val, OrderedDict):\n",
    "            self._unique += 1\n",
    "            key += \"_\"+str(self._unique)\n",
    "        OrderedDict.__setitem__(self, key, val)\n",
    "        \n",
    "def load_conv2caffe(buf, start, conv_param):  \n",
    "    weight = conv_param[0].data  \n",
    "    bias = conv_param[1].data  \n",
    "    conv_param[1].data[...] = np.reshape(buf[start:start+bias.size], bias.shape);   start = start + bias.size  \n",
    "    conv_param[0].data[...] = np.reshape(buf[start:start+weight.size], weight.shape); start = start + weight.size  \n",
    "    return start\n",
    "def load_fc2caffe(buf, start, fc_param):  \n",
    "    weight = fc_param[0].data  \n",
    "    bias = fc_param[1].data  \n",
    "    fc_param[1].data[...] = np.reshape(buf[start:start+bias.size], bias.shape);   start = start + bias.size  \n",
    "    fc_param[0].data[...] = np.reshape(buf[start:start+weight.size], weight.shape); start = start + weight.size  \n",
    "    return start\n",
    "def load_conv_bn2caffe(buf, start, conv_param, bn_param, scale_param): \n",
    "    conv_weight = conv_param[0].data  \n",
    "    running_mean = bn_param[0].data  \n",
    "    running_var = bn_param[1].data  \n",
    "    scale_weight = scale_param[0].data  \n",
    "    scale_bias = scale_param[1].data      \n",
    "    scale_param[1].data[...] = np.reshape(buf[start:start+scale_bias.size], scale_bias.shape); start = start + scale_bias.size  \n",
    "    #print scale_bias.size  \n",
    "    #print scale_bias  \n",
    "  \n",
    "    scale_param[0].data[...] = np.reshape(buf[start:start+scale_weight.size], scale_weight.shape); start = start + scale_weight.size  \n",
    "    #print scale_weight.size\n",
    "    #print scale_weight\n",
    "  \n",
    "    bn_param[0].data[...] = np.reshape(buf[start:start+running_mean.size], running_mean.shape); start = start + running_mean.size  \n",
    "    #print running_mean.size\n",
    "    #print running_mean\n",
    "  \n",
    "    bn_param[1].data[...] = np.reshape(buf[start:start+running_var.size], running_var.shape); start = start + running_var.size  \n",
    "    #print running_var.size\n",
    "    #print running_var\n",
    "  \n",
    "    bn_param[2].data[...] = np.array([1.0])  \n",
    "    conv_param[0].data[...] = np.reshape(buf[start:start+conv_weight.size], conv_weight.shape); start = start + conv_weight.size  \n",
    "    #print conv_weight.size\n",
    "    #print conv_weight\n",
    "  \n",
    "    return start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def darknet2caffe(cfgfile, weightfile, protofile, caffemodel='gene.caffemodel'):  \n",
    "    #net_info = cfg2prototxt(cfgfile)\n",
    "    #save_prototxt(net_info , protofile, region=False)  \n",
    "    print('benchmark')\n",
    "    net = caffe.Net(protofile, caffe.TEST)\n",
    "    k_v_s = [(k, v) for k, v in net.params.items()]\n",
    "    key_vecnums = [(vecs[0], len(vecs[1])) for vecs in k_v_s]\n",
    "    print([(vecs[0], [vecs[1][i].data.shape for i in range(len(vecs[1]))])for vecs in k_v_s])\n",
    "    print('benchmark')\n",
    "    params = net.params\n",
    "    print('benchmark')\n",
    "  \n",
    "    #blocks = parse_cfg(cfgfile)\n",
    "    parser = ConfigParser(dict_type=uniqdict)\n",
    "    parser.read(cfgfile)\n",
    "    blocks = parser.sections()\n",
    "    print(blocks)\n",
    "    \n",
    "  \n",
    "    #Open the weights file  \n",
    "    fp = open(weightfile, \"rb\")  \n",
    "  \n",
    "    #The first 4 values are header information   \n",
    "    # 1. Major version number  \n",
    "    # 2. Minor Version Number  \n",
    "    # 3. Subversion number   \n",
    "    # 4. IMages seen   \n",
    "    header = np.fromfile(fp, dtype = np.int32, count = 5)\n",
    "    #header = np.fromfile(fp, dtype = np.float32, count = 5)\n",
    "    print(header)\n",
    "    #fp = open(weightfile, 'rb')  \n",
    "    #header = np.fromfile(fp, count=5, dtype=np.int32)  \n",
    "    #header = np.ndarray(shape=(5,),dtype='int32',buffer=fp.read(20))  \n",
    "    #print(header)  \n",
    "    buf = np.fromfile(fp, dtype = np.float32)\n",
    "    print('buf len:{0}'.format(len(buf)))\n",
    "    #print(buf)  \n",
    "    fp.close()  \n",
    "  \n",
    "    layers = []  \n",
    "    layer_id = 1  \n",
    "    start = 0  \n",
    "    for block in blocks:\n",
    "        print(block)\n",
    "        if start >= buf.size:  \n",
    "            break\n",
    "        items = dict(parser.items(block))\n",
    "        print(items)\n",
    "        if block.split('_')[0] == 'net':  \n",
    "            continue\n",
    "        elif ((block.split('_')[0] == 'convolutional') or \n",
    "        (block.split('_')[0] == 'deconvolutional')):\n",
    "            batchnorm_followed = False\n",
    "            relu_followed = False\n",
    "            \n",
    "            if 'batch_normalize' in items and items['batch_normalize']:\n",
    "                batchnorm_followed = True\n",
    "            if 'activation' in items and items['activation'] != 'linear':\n",
    "                relu_followed = True\n",
    "            \n",
    "            if items.has_key('name'):  \n",
    "                conv_layer_name = items['name']  \n",
    "                print('has key name ' + conv_layer_name)\n",
    "                bn_layer_name = '%s-bn' % items['name']  \n",
    "                scale_layer_name = '%s-scale' % items['name']  \n",
    "            else:\n",
    "                if(block.split('_')[0] == 'deconvolutional'):\n",
    "                    conv_layer_name = 'layer%d-upsample' % layer_id  \n",
    "                    print('has no name ' + conv_layer_name)\n",
    "                    #bn_layer_name = 'layer%d-bn' % layer_id  \n",
    "                    #scale_layer_name = 'layer%d-scale' % layer_id \n",
    "                else:\n",
    "                    conv_layer_name = 'layer%d-conv' % layer_id  \n",
    "                    print('has no name ' + conv_layer_name)\n",
    "                    bn_layer_name = 'layer%d-bn' % layer_id  \n",
    "                    scale_layer_name = 'layer%d-scale' % layer_id  \n",
    "  \n",
    "            if batchnorm_followed:\n",
    "                print(\"load_conv_bn2caffe:\")\n",
    "                start = load_conv_bn2caffe(buf, start, params[conv_layer_name], \n",
    "                                           params[bn_layer_name], params[scale_layer_name])\n",
    "            else:\n",
    "                print(\"load_conv2caffe:\")\n",
    "                start = load_conv2caffe(buf, start, params[conv_layer_name])  \n",
    "            layer_id = layer_id+1\n",
    "            print('start:{0}'.format(start))\n",
    "        elif block.split('_')[0] == 'connected':  \n",
    "            if items.has_key('name'):  \n",
    "                fc_layer_name = items['name']  \n",
    "            else:  \n",
    "                fc_layer_name = 'layer%d-fc' % layer_id  \n",
    "            start = load_fc2caffe(buf, start, params[fc_layer_name])  \n",
    "            layer_id = layer_id+1  \n",
    "        elif block.split('_')[0] == 'maxpool':  \n",
    "            layer_id = layer_id+1  \n",
    "        elif block.split('_')[0] == 'avgpool':  \n",
    "            layer_id = layer_id+1  \n",
    "        elif block.split('_')[0] == 'region':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'route':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'shortcut':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'softmax':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'cost':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'upsample':  \n",
    "            layer_id = layer_id + 1 \n",
    "        elif block.split('_')[0] == 'yolo':  \n",
    "            layer_id = layer_id + 1\n",
    "        else:  \n",
    "            print('unknow layer type %s ' % block.split('_')[0]) \n",
    "            layer_id = layer_id + 1 \n",
    "    print('save caffemodel to %s' % caffemodel)  \n",
    "    net.save(caffemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgfile = 'yolov3.cfg'\n",
    "weights = 'yolov3.weights'\n",
    "prototxt = 'yolov3.prototxt'\n",
    "caffemodel = 'Jenerated_yolov3_aaa.caffemodel'\n",
    "darknet2caffe(cfgfile, weights, prototxt, caffemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
