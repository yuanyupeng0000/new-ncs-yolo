{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LPRNet:\n",
    "    # Function for generation characters range\n",
    "\n",
    "    # Fire block\n",
    "    @staticmethod\n",
    "    def fire_block(block_input, outputs):\n",
    "        \n",
    "        fire = slim.conv2d(block_input, outputs / 4, [1, 1])\n",
    "        fire = slim.conv2d(fire, outputs / 4, [3, 3])\n",
    "        fire = slim.conv2d(fire, outputs, [1, 1])\n",
    "        return fire\n",
    "\n",
    "    # Small Fire block\n",
    "    @staticmethod\n",
    "    def small_fire_block(block_input, outputs):\n",
    "        fire = slim.conv2d(block_input, outputs / 4, [1, 1])\n",
    "        fire = slim.conv2d(fire, outputs / 4, [3, 1])\n",
    "        fire = slim.conv2d(fire, outputs / 4, [1, 3])\n",
    "        fire = slim.conv2d(fire, outputs, [1, 1])\n",
    "        return fire\n",
    "\n",
    "    # Inception-ResNet like block\n",
    "    @staticmethod\n",
    "    def resinc_block(block_input, outputs):\n",
    "        inputs = int(block_input.get_shape()[3])\n",
    "        if inputs == outputs:\n",
    "            res = block_input\n",
    "        else:\n",
    "            res = slim.conv2d(block_input, outputs, [1, 1])\n",
    "        inc1 = slim.conv2d(block_input, outputs / 8, [1, 1])\n",
    "        inc2 = slim.conv2d(block_input, outputs / 8, [1, 1])\n",
    "        inc2 = slim.conv2d(inc2, outputs / 8, [3, 1])\n",
    "        inc2 = slim.conv2d(inc2, outputs / 8, [1, 3])\n",
    "        conc = tf.concat(3, [inc1, inc2])\n",
    "        inc = slim.conv2d(conc, outputs, [1, 1])\n",
    "        return res + inc\n",
    "\n",
    "    # basic_block = fire_block\n",
    "    basic_block = small_fire_block\n",
    "\n",
    "    # basic_block = resinc_block\n",
    "\n",
    "    # Convolution block for CNN\n",
    "    @staticmethod\n",
    "    def convolution_block(block_input, outputs, stride, **kwargs):\n",
    "        scope = kwargs.pop('scope', None)\n",
    "        # cr = slim.conv2d(input, outputs, [3, 3], scope=scope)\n",
    "        b_block = LPRNet.basic_block(block_input, outputs)\n",
    "        max_pool = slim.max_pool2d(b_block, [3, 3], stride=(stride, 1), padding='VALID', scope=scope)\n",
    "        return max_pool\n",
    "\n",
    "    @staticmethod\n",
    "    def enet_input_block(block_input, **kwargs):\n",
    "        scope = kwargs.pop('scope', None)\n",
    "        input1 = slim.conv2d(block_input, 61, [3, 3], stride=(2, 1), padding='VALID', scope=scope)\n",
    "        input2 = slim.avg_pool2d(block_input, [3, 3], stride=(2, 1), padding='VALID', scope=scope)\n",
    "        step1 = tf.concat(3, [input1, input2])\n",
    "        step2 = LPRNet.basic_block(step1, 128)\n",
    "        step2 = slim.max_pool2d(step2, [2, 2], stride=(1, 1), padding='VALID', scope=scope)\n",
    "        return step2\n",
    "\n",
    "    @staticmethod\n",
    "    def std_input_block(block_input):\n",
    "        return slim.stack(block_input, LPRNet.convolution_block, [(64, 1), (128, 2)])\n",
    "\n",
    "    @staticmethod\n",
    "    def mixed_input_block(block_input):\n",
    "        cnn = slim.conv2d(block_input, 64, [3, 3])\n",
    "        cnn = slim.max_pool2d(cnn, [3, 3], stride=(1, 1), padding='VALID')\n",
    "        cnn = LPRNet.basic_block(cnn, 128)\n",
    "        cnn = slim.max_pool2d(cnn, [3, 3], stride=(2, 1), padding='VALID')\n",
    "        return cnn\n",
    "\n",
    "    input_block = mixed_input_block\n",
    "\n",
    "    @staticmethod\n",
    "    def lprnet(net_input):\n",
    "        with slim.arg_scope([slim.fully_connected, slim.conv2d], activation_fn=tf.nn.relu,\n",
    "                        normalizer_fn=slim.batch_norm, weights_initializer=tf.truncated_normal_initializer(stddev=0.01),\n",
    "                        weights_regularizer=slim.l2_regularizer(0.0005)):\n",
    "            cnn = LPRNet.input_block(net_input)\n",
    "            cnn = LPRNet.basic_block(cnn, 256)\n",
    "            cnn = LPRNet.convolution_block(cnn, 256, 2)\n",
    "\n",
    "            cnn = slim.dropout(cnn)\n",
    "            cnn = slim.conv2d(cnn, 256, [4, 1], padding='VALID')\n",
    "            cnn = slim.dropout(cnn)\n",
    "\n",
    "        return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_array = np.zeros([1,24,94,3])\n",
    "empty_array = tf.cast(empty_array, tf.float32)\n",
    "cnn = LPRNet.lprnet(empty_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = slim.conv2d(cnn, 37, [1, 13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = slim.fully_connected(slim.flatten(classes), 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(cnn.get_shape()[2])\n",
    "pattern = tf.reshape(pattern, (-1, 1, 1, 128))\n",
    "pattern = tf.tile(pattern, [1, 1, 88, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-92b260b5ae95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# skip connection over RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalizer_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# fully convolutional linear activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0minf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_classes' is not defined"
     ]
    }
   ],
   "source": [
    "inf = tf.concat(axis=3, values=[classes, pattern])  # skip connection over RNN\n",
    "inf = slim.conv2d(inf, 37, [1, 1], normalizer_fn=None, activation_fn=None)  # fully convolutional linear activation\n",
    "inf = tf.squeeze(inf, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(1), Dimension(1), Dimension(88), Dimension(128)])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(1), Dimension(1), Dimension(88), Dimension(37)])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(1), Dimension(1), Dimension(88), Dimension(128)])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LPRVocab:\n",
    "    @staticmethod\n",
    "    def create_vocab(train_list_path, val_list_path, use_h_concat=False, use_oi_concat=False):\n",
    "        [vocab, r_vocab, num_classes] = LPRVocab._create_standard_vocabs(train_list_path, val_list_path)\n",
    "        if use_h_concat:\n",
    "            [vocab, r_vocab, num_classes] = LPRVocab._concat_all_hieroglyphs(vocab, r_vocab)\n",
    "        if use_oi_concat:\n",
    "            [vocab, r_vocab, num_classes] = LPRVocab._concat_oi(vocab, r_vocab)\n",
    "\n",
    "        return vocab, r_vocab, num_classes\n",
    "\n",
    "    @staticmethod\n",
    "    def _char_range(char1, char2):\n",
    "        \"\"\"Generates the characters from `char1` to `char2`, inclusive.\"\"\"\n",
    "        for char_code in range(ord(char1), ord(char2) + 1):\n",
    "            yield chr(char_code)\n",
    "\n",
    "    # Function for reading special symbols\n",
    "    @staticmethod\n",
    "    def _read_specials(filepath):\n",
    "        characters = set()\n",
    "        with open(filepath, 'r') as file_:\n",
    "            for line in file_:\n",
    "                current_label = line.split(' ')[-1].strip()\n",
    "                characters = characters.union(re.findall('(<[^>]*>|.)', current_label))\n",
    "        return characters\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_standard_vocabs(train_list_path, val_list_path):\n",
    "        chars = set().union(LPRVocab._char_range('A', 'Z')).union(LPRVocab._char_range('0', '9'))\n",
    "        chars = chars.union(LPRVocab._read_specials(train_list_path)).union(LPRVocab._read_specials(val_list_path))\n",
    "        chars = list(chars)\n",
    "        chars.sort()\n",
    "        chars.append('_')\n",
    "        num_classes = len(chars)\n",
    "        vocab = dict(zip(chars, range(num_classes)))\n",
    "        r_vocab = dict(zip(range(num_classes), chars))\n",
    "        r_vocab[-1] = ''\n",
    "        return [vocab, r_vocab, num_classes]\n",
    "\n",
    "    #Function for treating all hieroglyphs as 1 class\n",
    "    @staticmethod\n",
    "    def _concat_all_hieroglyphs(vocab_before, r_vocab_before):\n",
    "        chars = vocab_before.keys()\n",
    "        tf.logging.info(\"Old number of classes: {}\".format(len(chars)))\n",
    "\n",
    "        # Get all hieroglyphs from train and test\n",
    "        hieroglyphs = list((set(chars) - set(LPRVocab._char_range('A', 'Z'))) - set(LPRVocab._char_range('0', '9')))\n",
    "\n",
    "        tf.logging.info('Total hieroglyphs num: {}'.format(len(hieroglyphs)))\n",
    "        tf.logging.info('Vocabulary before: ')\n",
    "        tf.logging.info(vocab_before)\n",
    "\n",
    "        chars = list(set().union(LPRVocab._char_range('A', 'Z')).union(LPRVocab._char_range('0', '9')))\n",
    "        chars.sort()\n",
    "        new_num_classes = len(chars)\n",
    "        vocab_after = dict(zip(chars, range(new_num_classes)))\n",
    "        vocab_after.update(dict(zip(hieroglyphs, [new_num_classes] * len(hieroglyphs))))\n",
    "        vocab_after['_'] = new_num_classes + 1\n",
    "        new_num_classes += 2\n",
    "\n",
    "        tf.logging.info('Vocabulary after: ')\n",
    "        tf.logging.info(vocab_after)\n",
    "\n",
    "        tf.logging.info('Reverse vocabulary before: ')\n",
    "        tf.logging.info(r_vocab_before)\n",
    "\n",
    "        r_vocab_after = dict((v, k) for k, v in vocab_after.iteritems())\n",
    "        r_vocab_after[-1] = ''\n",
    "\n",
    "        tf.logging.info('Reverse vocabulary after: ')\n",
    "        tf.logging.info(r_vocab_after)\n",
    "\n",
    "        tf.logging.info('New number of classes: {}'.format(new_num_classes))\n",
    "        tf.logging.info('Vocabulary len: {}'.format(len(vocab_after)))\n",
    "        tf.logging.info('Reverse vocabulary length: {}'.format(len(r_vocab_after)))\n",
    "\n",
    "        return [vocab_after, r_vocab_after, new_num_classes]\n",
    "\n",
    "    #Function for treating O/0, I/1 as 1 class\n",
    "    @staticmethod\n",
    "    def _concat_oi(vocab_before, r_vocab_before):\n",
    "        chars = vocab_before.keys()\n",
    "        tf.logging.info('Old number of classes: {}'.format(len(chars)))\n",
    "\n",
    "        tf.logging.info('Vocabulary before: ')\n",
    "        tf.logging.info(vocab_before)\n",
    "\n",
    "        # Remove '0' and '1'\n",
    "        chars = list(set(chars) - set(['0', '1']))\n",
    "        chars.sort()\n",
    "        new_num_classes = len(chars)\n",
    "        vocab_after = dict(zip(chars, range(new_num_classes)))\n",
    "        vocab_after['0'] = vocab_after['O']\n",
    "        vocab_after['1'] = vocab_after['I']\n",
    "        vocab_after['_'] = new_num_classes + 1\n",
    "        new_num_classes += 1\n",
    "\n",
    "        tf.logging.info('Vocabulary after: ')\n",
    "        tf.logging.info(vocab_after)\n",
    "\n",
    "        tf.logging.info('Reverse vocabulary before: ')\n",
    "        tf.logging.info(r_vocab_before)\n",
    "\n",
    "        r_vocab_after = dict((v, k) for k, v in vocab_after.iteritems())\n",
    "        r_vocab_after[-1] = ''\n",
    "\n",
    "        tf.logging.info('Reverse vocabulary after: ')\n",
    "        tf.logging.info(r_vocab_after)\n",
    "\n",
    "        tf.logging.info('New number of classes: {}'.format(new_num_classes))\n",
    "        tf.logging.info('Vocabulary len: {}'.format(len(vocab_after)))\n",
    "        tf.logging.info('Reverse vocabulary length: {}'.format(len(r_vocab_after)))\n",
    "\n",
    "        return [vocab_after, r_vocab_after, new_num_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object LPRVocab._char_range at 0x7fb0ea040db0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LPRVocab._char_range('A', 'Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(ord('A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set().union(LPRVocab._char_range('A', 'Z')).union(LPRVocab._char_range('0', '9'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
